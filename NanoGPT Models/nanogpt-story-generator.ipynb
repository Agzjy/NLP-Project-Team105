{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9035085,"sourceType":"datasetVersion","datasetId":5435404}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport torch.optim as optim\nimport sentencepiece as spm\nimport os\nimport pandas as pd\nimport math\n\n# Hyperparameters\nbatch_size = 512\nblock_size = 32\nmax_iters = 5000\neval_interval = 500\nlearning_rate = 1e-4\nweight_decay = 1e-5  # Weight decay for regularization\npatience = 3  # Early stopping patience\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 512\nn_head = 4\nn_layer = 4\ndropout = 0.2\n\n# Load the Story Cloze dataset\nstory_2016 = pd.read_csv('/kaggle/input/story-cloze/cloze_2016.csv')\nstory_2016['story'] = story_2016['storytitle'] + ' ' + story_2016[['sentence1', 'sentence2', 'sentence3', 'sentence4', 'sentence5']].agg(' '.join, axis=1)\nstory_2018 = pd.read_csv('/kaggle/input/story-cloze/cloze_2017.csv')\nstory_2018['story'] = story_2018['storytitle'] + ' ' + story_2018[['sentence1', 'sentence2', 'sentence3', 'sentence4', 'sentence5']].agg(' '.join, axis=1)\nstory_cloze = pd.concat([story_2016, story_2018], axis=0)\n\n# with open('/kaggle/working/text_data.txt', 'w') as f:\n#     f.write('\\n'.join(story_cloze['story'].tolist()))\n# # Train SentencePiece model\n# spm.SentencePieceTrainer.train(input='/kaggle/working/text_data.txt', model_prefix='spm_model', vocab_size=10000, model_type='bpe')\n\n# Initialize SentencePiece tokenizer\nsp = spm.SentencePieceProcessor(model_file='/kaggle/working/spm_model.model')\n\n# Encode and decode functions using SentencePiece\ndef encode(text):\n    return sp.encode(text, out_type=int)\n\ndef decode(ids):\n    return sp.decode(ids)\n\n# Prepare data\ntext = ' '.join(story_cloze['story'])\ndata = torch.tensor(encode(text), dtype=torch.long).to(device)  # Move tensor to device\nn = int(0.8 * len(data))\ntrain_data = data[:n]\nval_data = data[n:]\n\n# Data loading\ndef get_batch(split):\n    data = train_data if split == 'train' else val_data\n    max_index = len(data) - block_size\n    if max_index <= 0:\n        raise ValueError(\"The block_size is too large for the given dataset. Consider reducing it.\")\n    ix = torch.randint(max_index, (batch_size,), device=device)\n    x_batches = []\n    y_batches = []\n    for i in ix:\n        end = min(i + block_size, len(data))\n        x = data[i:end]\n        y = data[i+1:end+1]\n        if len(x) < block_size:\n            x = torch.cat([x, torch.zeros(block_size - len(x), dtype=torch.long, device=device)])\n            y = torch.cat([y, torch.zeros(block_size - len(y), dtype=torch.long, device=device)])\n        x_batches.append(x)\n        y_batches.append(y)\n    x = torch.stack(x_batches).to(device)\n    y = torch.stack(y_batches).to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss_and_perplexity():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters, device=device)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        avg_loss = losses.mean()\n        perplexity = math.exp(avg_loss)\n        out[split] = (avg_loss, perplexity)\n    model.train()\n    return out\n\nclass Head(nn.Module):\n    \"\"\" One head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False).to(device)\n        self.query = nn.Linear(n_embd, head_size, bias=False).to(device)\n        self.value = nn.Linear(n_embd, head_size, bias=False).to(device)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size, device=device)))\n        self.dropout = nn.Dropout(dropout).to(device)\n\n    def forward(self, x):\n        B, T, C = x.shape\n        k = self.key(x)   # (B,T,hs)\n        q = self.query(x) # (B,T,hs)\n        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        v = self.value(x) # (B,T,hs)\n        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\" Multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(head_size * num_heads, n_embd).to(device)\n        self.dropout = nn.Dropout(dropout).to(device)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\nclass FeedForward(nn.Module):\n    \"\"\" A simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd).to(device),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd).to(device),\n            nn.Dropout(dropout).to(device),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size).to(device)\n        self.ffwd = FeedForward(n_embd).to(device)\n        self.ln1 = nn.LayerNorm(n_embd).to(device)\n        self.ln2 = nn.LayerNorm(n_embd).to(device)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\nclass GPTLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(sp.get_piece_size(), n_embd).to(device)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd).to(device)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head).to(device) for _ in range(n_layer)]).to(device)\n        self.ln_f = nn.LayerNorm(n_embd).to(device)\n        self.lm_head = nn.Linear(n_embd, sp.get_piece_size()).to(device)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        tok_emb = self.token_embedding_table(idx)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n        x = tok_emb + pos_emb\n        x = self.blocks(x)\n        x = self.ln_f(x)\n        logits = self.lm_head(x)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens=100):\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -block_size:]\n            logits, _ = self(idx_cond)\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat((idx, idx_next), dim=1)\n        return idx\n\nmodel = GPTLanguageModel().to(device)\nprint(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n\noptimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n\n#Checkpointing function\ndef save_checkpoint(model, optimizer, epoch, path, loss):\n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'best_val_loss' : loss\n    }, path)\n\n# Training loop with checkpointing\nbest_val_loss = float('inf')\ncheckpoint_path = 'model_checkpoint.pth'\nno_improvement_count = 0\n\n# Hyperparameters\nnum_epochs = 1  # Number of epochs\n\n# Training loop\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n    \n    for iter in range(max_iters):\n        if iter % eval_interval == 0 or iter == max_iters - 1:\n            losses_and_perplexities = estimate_loss_and_perplexity()\n            train_loss, train_perplexity = losses_and_perplexities['train']\n            val_loss, val_perplexity = losses_and_perplexities['val']\n\n            print(f\"Step {iter}: train loss {train_loss:.4f}, train perplexity {train_perplexity:.4f}, val loss {val_loss:.4f}, val perplexity {val_perplexity:.4f}\")\n\n            # Save the best model based on validation loss\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                save_checkpoint(model, optimizer, iter, checkpoint_path, best_val_loss)\n                no_improvement_count = 0\n            else:\n                no_improvement_count += 1\n\n            # Early stopping\n            if no_improvement_count >= patience:\n                print(\"Early stopping triggered.\")\n                break\n\n        xb, yb = get_batch('train')\n        logits, loss = model(xb, yb)\n        optimizer.zero_grad(set_to_none=True)\n        loss.backward()\n        optimizer.step()\n\n    # Print the epoch summary\n    print(f\"Epoch {epoch + 1} completed. Best validation loss: {best_val_loss:.4f}\")\n\n    # Early stopping after each epoch\n    if no_improvement_count >= patience:\n        print(\"Early stopping triggered after epoch.\")\n        break","metadata":{"execution":{"iopub.status.busy":"2024-08-11T17:57:55.500054Z","iopub.execute_input":"2024-08-11T17:57:55.500874Z","iopub.status.idle":"2024-08-11T18:44:07.654252Z","shell.execute_reply.started":"2024-08-11T17:57:55.500841Z","shell.execute_reply":"2024-08-11T18:44:07.652650Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"22.8708 M parameters\nEpoch 1/2\nStep 0: train loss 9.3822, train perplexity 11874.8537, val loss 9.3819, val perplexity 11871.8756\nStep 500: train loss 4.7635, train perplexity 117.1586, val loss 4.7887, val perplexity 120.1410\nStep 1000: train loss 4.3933, train perplexity 80.9081, val loss 4.4474, val perplexity 85.4054\nStep 1500: train loss 4.1505, train perplexity 63.4657, val loss 4.2368, val perplexity 69.1849\nStep 2000: train loss 3.9843, train perplexity 53.7467, val loss 4.0935, val perplexity 59.9486\nStep 2500: train loss 3.8605, train perplexity 47.4880, val loss 3.9956, val perplexity 54.3606\nStep 3000: train loss 3.7643, train perplexity 43.1328, val loss 3.9225, val perplexity 50.5275\nStep 3500: train loss 3.6876, train perplexity 39.9501, val loss 3.8651, val perplexity 47.7062\nStep 4000: train loss 3.6102, train perplexity 36.9722, val loss 3.8211, val perplexity 45.6522\nStep 4500: train loss 3.5541, train perplexity 34.9554, val loss 3.7846, val perplexity 44.0200\nStep 4999: train loss 3.4961, train perplexity 32.9860, val loss 3.7520, val perplexity 42.6045\nEpoch 1 completed. Best validation loss: 3.7520\nEpoch 2/2\nStep 0: train loss 3.4952, train perplexity 32.9552, val loss 3.7534, val perplexity 42.6662\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 254\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEarly stopping triggered.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 254\u001b[0m xb, yb \u001b[38;5;241m=\u001b[39m \u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m model(xb, yb)\n\u001b[1;32m    256\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","Cell \u001b[0;32mIn[12], line 67\u001b[0m, in \u001b[0;36mget_batch\u001b[0;34m(split)\u001b[0m\n\u001b[1;32m     65\u001b[0m x \u001b[38;5;241m=\u001b[39m data[i:end]\n\u001b[1;32m     66\u001b[0m y \u001b[38;5;241m=\u001b[39m data[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:end\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m<\u001b[39m block_size:\n\u001b[1;32m     68\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, torch\u001b[38;5;241m.\u001b[39mzeros(block_size \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(x), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)])\n\u001b[1;32m     69\u001b[0m     y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([y, torch\u001b[38;5;241m.\u001b[39mzeros(block_size \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(y), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)])\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:964\u001b[0m, in \u001b[0;36mTensor.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[38;5;21m__neg__\u001b[39m \u001b[38;5;241m=\u001b[39m _C\u001b[38;5;241m.\u001b[39m_TensorBase\u001b[38;5;241m.\u001b[39mneg\n\u001b[1;32m    962\u001b[0m \u001b[38;5;21m__abs__\u001b[39m \u001b[38;5;241m=\u001b[39m _C\u001b[38;5;241m.\u001b[39m_TensorBase\u001b[38;5;241m.\u001b[39mabs\n\u001b[0;32m--> 964\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    966\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__len__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"def generate_story(prompt, num_sentences=5):\n    context = torch.tensor(encode(prompt), dtype=torch.long, device=device).unsqueeze(0)\n    generated = model.generate(context, max_new_tokens=500)\n    generated_text = decode(generated[0].tolist())\n    sentences = generated_text.split('. ')\n    return '.\\n'.join(sentences[:num_sentences]) + '.'\n\nprint(\"'Good doctor'\")\nprint(generate_story('Good doctor'))","metadata":{"execution":{"iopub.status.busy":"2024-08-11T18:49:20.496241Z","iopub.execute_input":"2024-08-11T18:49:20.496914Z","iopub.status.idle":"2024-08-11T18:49:25.059645Z","shell.execute_reply.started":"2024-08-11T18:49:20.496881Z","shell.execute_reply":"2024-08-11T18:49:25.058557Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"'Good doctor'\nGood doctor Sally was driving one night down the street.\nSuddenly she saw a pup.\nShe gave some needles but it was my reason.\nSo when she saw the strained out, I was smashing it out.\nShe ran away and eventually asked to be out in talk.\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"'Little girl'\")\nprint(generate_story('Little Girl'))","metadata":{"execution":{"iopub.status.busy":"2024-08-11T18:51:00.618433Z","iopub.execute_input":"2024-08-11T18:51:00.619392Z","iopub.status.idle":"2024-08-11T18:51:05.418727Z","shell.execute_reply.started":"2024-08-11T18:51:00.619344Z","shell.execute_reply":"2024-08-11T18:51:05.417654Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"'Little girl'\nLittle Girl Recini Joce Jill entered a baking competition.\nFirst, she was waiting for her start.\nIt was Halloween and Handleg careless.\nShe saw a scary bed of clowns.\nWhen the sun was there when her mom found out her.\n","output_type":"stream"}]},{"cell_type":"code","source":"print(generate_story('Good doctor'))","metadata":{"execution":{"iopub.status.busy":"2024-08-11T18:58:01.739938Z","iopub.execute_input":"2024-08-11T18:58:01.740364Z","iopub.status.idle":"2024-08-11T18:58:06.069999Z","shell.execute_reply.started":"2024-08-11T18:58:01.740330Z","shell.execute_reply":"2024-08-11T18:58:06.069014Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Good doctor for the countless appointment The doctor told Tina his workout patches were remodeling there.\nKate decided not to wear sunscreen.\nShe pulled out her trip to Mexico.\nEveryone had a great day at work.\nShe was so happy to be on the roller coaster for a long time.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load the model and optimizer state\ndef load_checkpoint(model, optimizer, path):\n    checkpoint = torch.load(path)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    epoch = checkpoint['epoch']\n    best_val_loss = checkpoint['best_val_loss']\n    return model, optimizer, epoch, best_val_loss\n\n# Example usage before resuming training or inference\nmodel = GPTLanguageModel().to(device)\noptimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\nfinal_model_path = '/kaggle/working/model_checkpoint.pth'\n\n# Load the saved model\nmodel, optimizer, start_epoch, best_val_loss = load_checkpoint(model, optimizer, final_model_path)\nprint(f\"Model loaded from {final_model_path}, starting from epoch {start_epoch} with best validation loss {best_val_loss:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-11T16:14:19.376320Z","iopub.execute_input":"2024-08-11T16:14:19.377161Z","iopub.status.idle":"2024-08-11T16:14:21.481005Z","shell.execute_reply.started":"2024-08-11T16:14:19.377123Z","shell.execute_reply":"2024-08-11T16:14:21.479840Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Model loaded from /kaggle/working/model_checkpoint.pth, starting from epoch 6500 with best validation loss 3.9046\n","output_type":"stream"}]},{"cell_type":"markdown","source":"NANOGPT with pretrained inputs\n\n|\n\n|\n\nv","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport torch.optim as optim\nimport sentencepiece as spm\nimport os\nimport pandas as pd\nimport math\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\n\n# Hyperparameters\nbatch_size = 32  # Increased batch size\nblock_size = 8\nmax_iters = 5000  # Reduced iterations\neval_interval = 1000  # Increased eval interval\nlearning_rate = 1e-4\nweight_decay = 1e-5\npatience = 3\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 50  # Reduced eval iterations\nn_embd = 16\nn_head = 2\nn_layer = 2\ndropout = 0.0\n\n# Load the Story Cloze dataset\nstory_2016 = pd.read_csv('/kaggle/input/story-cloze/cloze_2016.csv')\nstory_2016['story'] = story_2016['storytitle'] + ' ' + story_2016[['sentence1', 'sentence2', 'sentence3', 'sentence4', 'sentence5']].agg(' '.join, axis=1)\nstory_2018 = pd.read_csv('/kaggle/input/story-cloze/cloze_2017.csv')\nstory_2018['story'] = story_2018['storytitle'] + ' ' + story_2018[['sentence1', 'sentence2', 'sentence3', 'sentence4', 'sentence5']].agg(' '.join, axis=1)\nstory_cloze = pd.concat([story_2016, story_2018], axis=0)\n\n# Initialize SentencePiece tokenizer\nsp = spm.SentencePieceProcessor(model_file='/kaggle/working/spm_model.model')\n\n# Encode and decode functions using SentencePiece\ndef encode(text):\n    return sp.encode(text, out_type=int)\n\ndef decode(ids):\n    return sp.decode(ids)\n\n# Prepare data\ntext = ' '.join(story_cloze['story'])\ndata = torch.tensor(encode(text), dtype=torch.long).to(device)\nn = int(0.8 * len(data))\ntrain_data = data[:n]\nval_data = data[n:]\n\n# Data loading\ndef get_batch(split):\n    data = train_data if split == 'train' else val_data\n    max_index = len(data) - block_size\n    if max_index <= 0:\n        raise ValueError(\"The block_size is too large for the given dataset. Consider reducing it.\")\n    ix = torch.randint(max_index, (batch_size,), device=device)\n    x_batches = []\n    y_batches = []\n    for i in ix:\n        end = min(i + block_size, len(data))\n        x = data[i:end]\n        y = data[i+1:end+1]\n        if len(x) < block_size:\n            x = torch.cat([x, torch.zeros(block_size - len(x), dtype=torch.long, device=device)])\n            y = torch.cat([y, torch.zeros(block_size - len(y), dtype=torch.long, device=device)])\n        x_batches.append(x)\n        y_batches.append(y)\n    x = torch.stack(x_batches).to(device)\n    y = torch.stack(y_batches).to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss_and_perplexity():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters, device=device)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        avg_loss = losses.mean()\n        perplexity = math.exp(avg_loss)\n        out[split] = (avg_loss, perplexity)\n    model.train()\n    return out\n\nclass Head(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False).to(device)\n        self.query = nn.Linear(n_embd, head_size, bias=False).to(device)\n        self.value = nn.Linear(n_embd, head_size, bias=False).to(device)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size, device=device)))\n        self.dropout = nn.Dropout(dropout).to(device)\n\n    def forward(self, x):\n        B, T, C = x.shape\n        k = self.key(x)\n        q = self.query(x)\n        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        wei = F.softmax(wei, dim=-1)\n        wei = self.dropout(wei)\n        v = self.value(x)\n        out = wei @ v\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(head_size * num_heads, n_embd).to(device)\n        self.dropout = nn.Dropout(dropout).to(device)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\nclass FeedForward(nn.Module):\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd).to(device),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd).to(device),\n            nn.Dropout(dropout).to(device),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size).to(device)\n        self.ffwd = FeedForward(n_embd).to(device)\n        self.ln1 = nn.LayerNorm(n_embd).to(device)\n        self.ln2 = nn.LayerNorm(n_embd).to(device)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\nclass GPTLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(sp.get_piece_size(), n_embd).to(device)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd).to(device)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head).to(device) for _ in range(n_layer)]).to(device)\n        self.ln_f = nn.LayerNorm(n_embd).to(device)\n        self.lm_head = nn.Linear(n_embd, sp.get_piece_size()).to(device)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        tok_emb = self.token_embedding_table(idx)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n        x = tok_emb + pos_emb\n        x = self.blocks(x)\n        x = self.ln_f(x)\n        logits = self.lm_head(x)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens=100):\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -block_size:]\n            logits, _ = self(idx_cond)\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat((idx, idx_next), dim=1)\n        return idx\n\nmodel = GPTLanguageModel().to(device)\nprint(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n\noptimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n\n# Checkpointing function\ndef save_checkpoint(model, optimizer, epoch, path, loss):\n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'loss': loss,\n    }, path)\n\n# Training with mixed precision\nscaler = torch.cuda.amp.GradScaler()\nbest_val_loss = float('inf')\nearly_stop_counter = 0\n\nfor iter in range(max_iters):\n    if iter % eval_interval == 0:\n        losses = estimate_loss_and_perplexity()\n        print(f\"Step {iter}: train loss {losses['train'][0]:.4f}, val loss {losses['val'][0]:.4f}, val perplexity {losses['val'][1]:.4f}\")\n\n        if losses['val'][0] < best_val_loss:\n            best_val_loss = losses['val'][0]\n            save_checkpoint(model, optimizer, iter, \"best_model.pt\", best_val_loss)\n            early_stop_counter = 0\n        else:\n            early_stop_counter += 1\n            if early_stop_counter >= patience:\n                print(\"Early stopping.\")\n                break\n\n    model.train()\n    xb, yb = get_batch('train')\n\n    optimizer.zero_grad()\n    with torch.cuda.amp.autocast():\n        logits, loss = model(xb, yb)\n\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n\ncontext = \"Good doctor\"\ninput_ids = torch.tensor(encode(context), dtype=torch.long).unsqueeze(0).to(device)\ngenerated_ids = model.generate(input_ids, max_new_tokens=100)[0].tolist()\ngenerated_text = decode(generated_ids)\nprint(generated_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-08T01:50:57.796926Z","iopub.execute_input":"2024-08-08T01:50:57.797348Z","iopub.status.idle":"2024-08-08T01:52:14.520159Z","shell.execute_reply.started":"2024-08-08T01:50:57.797319Z","shell.execute_reply":"2024-08-08T01:52:14.518816Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"0.336624 M parameters\nStep 0: train loss 9.2110, val loss 9.2115, val perplexity 10011.1559\nStep 1000: train loss 7.6166, val loss 7.6235, val perplexity 2045.6615\nStep 2000: train loss 6.6963, val loss 6.6888, val perplexity 803.3978\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 237\u001b[0m\n\u001b[1;32m    234\u001b[0m         logits, loss \u001b[38;5;241m=\u001b[39m model(xb, yb)\n\u001b[1;32m    236\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 237\u001b[0m     \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m    240\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGood doctor\u001b[39m\u001b[38;5;124m\"\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:416\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    414\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 416\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:315\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m--> 315\u001b[0m     retval \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adamw.py:184\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    171\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    174\u001b[0m         group,\n\u001b[1;32m    175\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    181\u001b[0m         state_steps,\n\u001b[1;32m    182\u001b[0m     )\n\u001b[0;32m--> 184\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adamw.py:335\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 335\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adamw.py:601\u001b[0m, in \u001b[0;36m_multi_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    599\u001b[0m     exp_avg_sq_sqrt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_foreach_sqrt(device_exp_avg_sqs)\n\u001b[0;32m--> 601\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_div_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg_sq_sqrt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_add_(exp_avg_sq_sqrt, eps)\n\u001b[1;32m    603\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_addcdiv_(device_params, device_exp_avgs, exp_avg_sq_sqrt, step_size)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"def generate_story(prompt, num_sentences=5):\n    context = torch.tensor(encode(prompt), dtype=torch.long, device=device).unsqueeze(0)\n    generated = model.generate(context, max_new_tokens=500)\n    generated_text = decode(generated[0].tolist())\n    sentences = generated_text.split('. ')\n    return '. '.join(sentences[:num_sentences]) + '.'\n\nprint(generate_story('Good doctor'))","metadata":{},"execution_count":null,"outputs":[]}]}