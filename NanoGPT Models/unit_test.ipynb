{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4558658,"sourceType":"datasetVersion","datasetId":2660706},{"sourceId":94364,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":79103,"modelId":103609}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import unittest\nimport torch\nimport math\n\n# assuming the full model code has already been written or imported above\nclass TestBigramLanguageModel(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        global device\n        device = torch.device('cpu')  # Force CPU usage for testing\n        \n        # Load the pre-trained model\n        cls.model = BigramLanguageModel()\n        model_path = '/kaggle/input/lyrics-generator-by-spotify-million-song-dataset/pytorch/default/1/lyric_generator_model.pth'\n        cls.model.load_state_dict(torch.load(model_path, map_location=device))\n        cls.model = cls.model.to(device)\n        \n        # Sample text for testing\n        cls.text = \"This is a sample text for testing purposes.\"\n        \n        # Tokenize text using the model's vocabulary\n        cls.data, cls.stoi, cls.itos, cls.vocab_size, cls.encode, cls.decode = cls.text_tokenize(cls.text, cls.model)\n\n        # Split data\n        cls.train_data, cls.val_data = train_test_split(cls.data, 0.8)\n\n    @staticmethod\n    def text_tokenize(text, model):\n        # Use the model's vocab size\n        stoi = model.token_embedding_table.weight.size(0)\n        itos = {i: ch for i, ch in enumerate(sorted(set(text)))}\n        stoi = {ch: i for i, ch in enumerate(sorted(set(text)))}\n\n        encode = lambda s: [stoi[c] for c in s if c in stoi]\n        decode = lambda l: ''.join([itos[i] for i in l if i in itos])\n\n        data = torch.tensor(encode(text), dtype=torch.long)\n\n        return data, stoi, itos, len(stoi), encode, decode\n\n    def test_model_initialization(self):\n        \"\"\"Test if the model initializes correctly.\"\"\"\n        self.assertIsInstance(self.model, BigramLanguageModel)\n        # Adjust to match the actual model's vocabulary size (77 in this case)\n        self.assertEqual(self.model.token_embedding_table.num_embeddings, 77)\n\n    def test_forward_pass(self):\n        \"\"\"Test the forward pass of the model.\"\"\"\n        x, y = get_batch('train')\n        logits, loss = self.model(x, y)\n        self.assertEqual(logits.shape, (batch_size * block_size, self.model.token_embedding_table.num_embeddings))\n        self.assertIsInstance(loss.item(), float)\n\n    def test_generate(self):\n        \"\"\"Test the text generation functionality.\"\"\"\n        context = torch.zeros((1, 1), dtype=torch.long, device=device)\n        generated = self.model.generate(context, max_new_tokens=10)\n        self.assertEqual(generated.shape, (1, 11))\n\n    def test_save_load_model(self):\n        \"\"\"Test saving and loading the model.\"\"\"\n        model_path = 'test_model.pth'\n        torch.save(self.model.state_dict(), model_path)\n        loaded_model = BigramLanguageModel()\n        loaded_model.load_state_dict(torch.load(model_path, map_location=device))\n        loaded_model = loaded_model.to(device)\n\n        x, y = get_batch('train')\n        logits_before, loss_before = self.model(x, y)\n        logits_after, loss_after = loaded_model(x, y)\n        self.assertTrue(torch.allclose(logits_before, logits_after))\n        self.assertAlmostEqual(loss_before.item(), loss_after.item())\n\n    def test_estimate_loss(self):\n        \"\"\"Test the loss estimation function.\"\"\"\n        losses = estimate_loss()\n        self.assertIn('train', losses)\n        self.assertIn('val', losses)\n        self.assertIsInstance(losses['train'].item(), float)\n        self.assertIsInstance(losses['val'].item(), float)\n\n# Run the tests\nunittest.main(argv=[''], verbosity=2, exit=False)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T21:58:56.266319Z","iopub.execute_input":"2024-08-15T21:58:56.266778Z","iopub.status.idle":"2024-08-15T21:58:56.618045Z","shell.execute_reply.started":"2024-08-15T21:58:56.266747Z","shell.execute_reply":"2024-08-15T21:58:56.616738Z"},"trusted":true},"execution_count":12,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01munittest\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbigram_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BigramLanguageModel, train_test_split, get_batch, calculate_perplexity, estimate_loss\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTestBigramLanguageModel\u001b[39;00m(unittest\u001b[38;5;241m.\u001b[39mTestCase):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetUpClass\u001b[39m(\u001b[38;5;28mcls\u001b[39m):\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bigram_model'"],"ename":"ModuleNotFoundError","evalue":"No module named 'bigram_model'","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}